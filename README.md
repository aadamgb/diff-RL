# Differentiable Learning for 2D Bicopter Control

## Description

A PyTorch-based differentiable framework for training neural network policies to control a 2D bicopter. The project supports three control modes [(SRT, CTBR, LV)](#control-modes) and includes real-time visualization in Pygame. Training can be executed in parallel environments at the GPU for efficient data colletion.



<div align="center">

![Bicopter Control Visualization](/media/sim.gif)

</div>



## Project Structure

```
â”œâ”€â”€ cfg/
â”‚   â””â”€â”€ dynamics/bicopter.yaml        # Physics params (not used for now)
â”œâ”€â”€ dynamics/
â”‚   â””â”€â”€ bicopter_dynamics.py          # 2D bicopter physics model
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ nn.py                         # Neural network policy
â”‚   â”œâ”€â”€ rand_traj_gen.py              # Harmonic trajectory generator
â”‚   â”œâ”€â”€ renderer.py                   # Pygame visualization
â”œâ”€â”€ train.py                          # Training script
â”œâ”€â”€ test_model.py                     # Policy evaluation & visualization
â””â”€â”€ outputs/                          # Saved policies 
```

## Overview
### ğŸ“‰Loss 

The bicopter's objective is to track a reference trajectory defined by position $p_{ref}$, velocity $v_{ref}$, and acceleration $a_{ref}$. The reference trajectory is generated by summing random sinusoidal harmonics, providing diverse training signals. The policy is trained by minimizing a weighted loss function:

$$\mathcal{L} = \mathcal{L}_{pos} + \mathcal{L}_{vel} + 0.25 \mathcal{L}_{\omega}$$


where the individual loss terms are:

$$\mathcal{L}_{pos} = \frac{1}{T} \sum_{t=1}^{T} \left\| \mathbf{p}_{t} - \mathbf{p}_{ref,t} \right\|^2 \quad \mathcal{L}_{vel} = \frac{1}{T} \sum_{t=1}^{T} \left\| \mathbf{v}_{t} - \mathbf{v}_{ref,t} \right\|^2 \quad \mathcal{L}_{\omega} = \frac{1}{T} \sum_{t=1}^{T} \omega_{t}^{2}$$

Here, $T$ is the optimization horizon over which loss is accumulated. Training uses truncated backpropagation through time (T-BPTT) with the ADAM optimizer at a learning rate of $1 \times 10^{-3}$.

### ğŸ§ Neural Network
The neural network (policy) is constructed in [`utils/nn.py`](utils/nn.py) as a simple feedforward multi-layer perceptron (MLP). The policy processes 9 observational inputs representing tracking errors, reference accelerations, and vehicle state:

| Position error| Velocity error | Acceleration reference | Orientiation and body rate|
|:-:|:-:|:-:|:-:|
| $e_{px} = x_{ref} - x$ | $e_{vx} = v_{x,ref} - v_x$| $a_{x,ref}$ | $\sin{\theta}, \ \cos{\theta}$
| $e_{py} = y_{ref} - y$| $e_{vy} = v_{y,ref} - v_y$ | $a_{y,ref}$ | $\omega$  |


#### Observation Input
Hence the observation input is:

$$\mathbf{o} =
\begin{bmatrix}
e_{px} &
e_{py} &
e_{vx} &
e_{vy} &
a_{x,ref} &
a_{y,ref} &
\sin(\theta) &
\cos(\theta) &
\omega
\end{bmatrix}^T \ \in \mathcal{R}^9
$$

#### Control Modes
The supported action outputs are:

- Single rotor thrust (SRT)
    - $T1, \ T2$
- Colective thrust and body rate (CTBR)
    - $T, \ \omega$ 
- Linear velocities + geometric controller gains (LV)
    - $v_x,\ v_y,\ kv,\ kR,\ kw $


### ğŸšBicopter Dynamics 
The physical bicopter system is modeled in [`dynamics/bicopter_dynamics.py`](dynamics/bicopter_dynamics.py) as a 6-state rigid body with two independent thrusters:

$$\dot{x} = v_x$$

$$\dot{y} = v_y$$

$$\dot{v}_x = -\frac{T_1 + T_2}{m}\sin\theta$$

$$\dot{v}_y = \frac{T_1 + T_2}{m}\cos\theta - g$$

$$\dot{\theta} = \omega$$

$$\dot{\omega} = \frac{l}{I}(T_2 - T_1)$$

where $m$ is the vehicle mass, $l$ is the rotor arm length, $I$ is the moment of inertia, $g$ is gravity, and $T_1, T_2$ are the individual rotor thrusts. The actuators are considered ideal with no saturation, and aerodynamic drag is neglected.

### ğŸ‹ï¸Training

The training logic is implemented in [`train.py`](train.py). The framework supports training on both CPU and GPU with parallel environments for efficient data collection. Training runs for a configurable number of episodes, each containing a specified number of steps. The optimization horizon $T$ is user-configurable and determines when the acumulated loss $\mathcal{L}$ is backpropageted through time. 


## Usage

### Training
```bash
python train.py
```
Trains a policy for the control mode specified by the parameter [`cm`](https://github.com/aadamgb/diff-RL/blob/2d501c9cc4309554f6ee41fb654d6027d55d48af/train.py#L26). The model will be saved in `outputs/{cm}.pt`.

### Evaluation & Visualization
To evaluate the policy, add it to this [dict](https://github.com/aadamgb/diff-RL/blob/2d501c9cc4309554f6ee41fb654d6027d55d48af/test_model.py#L87), then run:
```bash
python test_model.py
```
It will load the policies and render them in Pygame. Multiple policies can be loaded simultaneously. 

## Requirements

- PyTorch
- Pygame
- NumPy
- Matplotlib
